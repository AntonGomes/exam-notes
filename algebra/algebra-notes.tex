\documentclass[a4paper, 10pt]{article}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage[margin=0.5cm]{geometry}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}  % For including images
\usepackage{enumitem}



\setlength{\parindent}{0pt}

\begin{document}
\begin{multicols*}{3}
\textbf{Fields}\\
A field is a non-zero commutative division ring. 
- All fields are Integral Domains. \\

\textbf{Divsion Rings}\\
A ring $F$ in which every non-zero element $a \in F$ has an inverse $a^{-1} \in F$ ($\implies$ has no zero-divsors!)

\textbf{Vector Spaces}\\
A vector space $V$ over a Field $F$ is any set equipped with vector addition and scalr multiplication.
$\forall\quad u, v, w \in V \text{ and } a,b\in F$
\begin{align*}
\mathbf{u}+(\mathbf{v}+\mathbf{w}) &= (\mathbf{u}+\mathbf{v})+\mathbf{w} \\
\mathbf{v}+\mathbf{w} &= \mathbf{w}+\mathbf{v} \\
\mathbf{v}+0 &= \mathbf{v} \\
\mathbf{v}+(-\mathbf{v}) &= 0 \\
a(\mathbf{v}+\mathbf{w}) &= a \mathbf{v}+a \mathbf{w} \\
(a+b) \mathbf{v} &= a \mathbf{v}+b \mathbf{v} \\
a(b \mathbf{v}) &= (a b) \mathbf{v} \\
1 v &= v 
\end{align*}

\textbf{Vector Subspaces}\\
A subset $U$ of a vector space $V$ is called a vector subspace if $U$ contains the zero vector and whenever $\bf{u}, \bf{v}\in U$ and $\lambda\in F$ we have \\
- $\bf{u} + \bf{v} \in U$\\
- $\lambda\bf{u}\in U$\\
We write $U\subseteq V$.\\
For infinite and finite $U_{1}, U_{2}\subseteq V$\\
- $U_{1}\cap U_{2}$  is a subspace\\
- $U_{1}+U_{2}$  is a subspace\\
- $U_{1}\cup U_{2}$  is not a subspace\\
- $\dim(U) \leq \dim V$\\
QUICK CHECK: For  $\bf{u}, \bf{v}\in U$ and $\lambda_{1}\lambda_{2}\in F$ we have $\lambda_{1}\bf{u}+\lambda_{2}\bf{v}\in U$

\textbf{Generating Vector Subspaces}\\
Let $T$ be a subset of a vector space $V$ over a field $F$. Then amongst all vector subspaces of $V$ that include $T$ there is a smallest vector subspace
$$
\langle T\rangle= \langle T\rangle_F \subseteq V .
$$
It can be described as the set of all vectors $\alpha_1 \vec{v}_1+\cdots+\alpha_r \vec{v}_r$ with $\alpha_1, \ldots, \alpha_r \in F$ and $\vec{v}_1, \ldots, \vec{v}_r \in T$, together with the zero vector in the case $T=\emptyset$.\\
A subset of a vector space is called a generating or spanning set of our vector space if its span is all of the vector space.

\textbf{Power Sets}\\
If $X$ is a set, then the set of all subsets $\mathcal{P}(X)=\{U: U \subseteq X\}$ of $X$ is the power set of $X$. 
A subset of $\mathcal{P}(X)$ is a system of subsets of $X$. 
Given such a system $\mathcal{U} \subseteq \mathcal{P}(X)$ we can create two new subsets of $X$, the union and the intersection of the sets of our system $\mathcal{U}$, as follows:
\begin{align*}
& \bigcup_{U \in \mathcal{U}} U=\{x \in X: \text {s.t. } \exists U \in \mathcal{U} \text { with } x \in U\} \\
& \bigcap_{U \in \mathcal{U}} U=\{x \in X: x \in U \text { for all } U \in \mathcal{U}\}
\end{align*}
In particular the intersection of the empty system of subsets of $X$ is $X$, and the union of the empty system of subsets of $X$ is the empty set.

\textbf{Linear Independence}\\
A subset $L$ of a vector space $V$ is called linearly independent if for all pairwise different vectors $\vec{v}_1, \ldots, \vec{v}_r \in L$ and arbitrary scalars $\alpha_1, \ldots, \alpha_r \in F$,
$$
\alpha_1 \vec{v}_1+\cdots+\alpha_r \vec{v}_r=\overrightarrow{0} \Longrightarrow \alpha_1=\cdots=\alpha_r=0
$$

\textbf{Basis}\\
A basis of a vector space V is a linearly independent generating set in V.\\
A basis always exists for finite vector spaces. 
The following are equivalent for a subset $E\subset V$:\\
(1) $E$ is a basis
(2) $E$ is minimal among all generating sets, meaning that $E \backslash\{\vec{v}\}$ does not generate $V$, for any $\vec{v} \in E$;\\
(3) $E$ is maximal among all linearly independent subsets, meaning that $E \cup\{\vec{v}\}$ is not linearly independent for any $\vec{v} \in V$.\\
(4) If $L \subset V$ is a linearly independent subset and $E$ is minimal amongst all generating sets of our vector space with the property that $L \subseteq E$, then $E$ is a basis.\\
(5) If $E \subseteq V$ is a generating set and if $L$ is maximal amongst all linearly independent subsets of vector space with the property $L \subseteq E$, then $L$ is a basis.

\textbf{Dimension}\\
The dimension of a vector space $V$ is the cardinality (size) of a basis of $V$.

\textbf{Fundamental Estimate}\\
No linearly independent subset of $a$ given vector space has more elements than a generating set. Thus if $V$ is a vector space, $L \subset V$ a linearly independent subset and $E \subseteq V$ a generating set, then:
$$
|L| \leqslant|E|
$$

\textbf{Steinitz Exchange Lemma}\\
Let $V$ be a vector space, $L \subset V$ a finite linearly independent subset and $E \subseteq V$ a generating set. Then there is an injection $\phi: L \hookrightarrow E$ such that $(E \backslash \phi(L)) \cup L$ is also a generating set for $V$.\\
In other words, we can swap some elements of a generating set by the elements of our linearly independent set, and still keep a generating set.

\textbf{Dimension Theorem}\\
Let $V$ be a vector space containing vector subspaces $U, W \subseteq V$. Then
$$
\operatorname{dim}(U+W)+\operatorname{dim}(U \cap W)=\operatorname{dim} U+\operatorname{dim} W .
$$

\textbf{Linear Homomorphisms}\\
Let $V$ and $W$ be vector spaces over the same field. \\
A function $f:V\rightarrow W$ is said to be a linear map if for all $x,y\in V$ and some scalar $c\in K$,
\begin{align*}
f(x+y) &= f(x)+f(y)\\
f(cx) &= cf(u)
\end{align*}
- A linear map is injective if and only if its kernel is zero. \\
- All linear maps have that $f(\vec{0})=\vec{0}$.\\
- Compositions of linear maps are also linear.\\
- Linear mappings are completely determined by the values they take on the basis of $V$. \\
Endomorphisms are Homomorphisms from a vector space to itself.\\
isomorphisms are bijective homomorphisms.\\
Automorphisms are isomorphisms from a vector space to itself.

\textbf{Kernal and Image}\\
The kernel of the linear mapping $f:V\rightarrow W$ is
$$
\operatorname{ker}(f):=f^{-1}(0)=\{v \in V: f(v)=0\}
$$
The image is the subset $$\operatorname{im}(f)=f(V) \subseteq W$$ 
The kernel and image are vector subspaces of $V$.

\textbf{Fixed Points}\\
Given a mapping $f: X \rightarrow X$, we denote the set of fixed points by
$$
X^f=\{x \in X: f(x)=x\} .
$$

\textbf{Complemetary Subspaces}\\
Two vector subspaces $V_1, V_2$ of a vector space $V$ are called complementary if $V_1 \times V_2 \stackrel{\sim}{\rightarrow} V$ (addition) defines a Bijection.

\textbf{Internal Direct Sum}\\
Given Complementary Subspaces $U, U^{\prime}\subseteq V$ and the Linear Mappings $f:U\rightarrow V$, $f^{\prime}:U^{\prime}\rightarrow V$ 
we then produce an vector space isomorphism $U \oplus U^\prime \stackrel{\sim}{\rightarrow} V$. 
$$f\left(u,u^{\prime}\right)=f\left(u\right)+ f^{\prime}\left(u^\prime\right)$$
We write $V=U \oplus U^{\prime}$ and say $V$ is the internal direct sum of $U$ and $U^\prime$.

\textbf{Direct Sum}\\
Let $V$ be a Vector Space with Vector Subspaces $V_1, \ldots, V_n$. \\
The vector subspace of $V$ they generate is called the sum of our vector subspaces and denoted by $V_1+\cdots+V_n$. \\
$$\langle V_1\cup\cdots\cup V_n\rangle=V_1+\cdots+V_n$$
If the homomorphism given by addition $V_1+\cdots+V_n \rightarrow V$ is an injection then we say the sum of the vector subspaces $V_i$ is direct.\\
We write their sum also as $V_1 \oplus \cdots \oplus V_n$.

\textbf{Linear Mapping and Basis}\\
Let $V, W$ be vector spaces over $F$ and let $B \subset V$ be a basis. Then restriction of a mapping gives a bijection
\begin{align*}
\operatorname{Hom}_F(V, W) & \stackrel{\sim}{\rightarrow} \operatorname{Maps}(B, W) \\
f & \left.\mapsto f\right|_B .
\end{align*}

\textbf{In-Bi-Surjection}\\
$f: A \rightarrow B$\\
- is an injection (or one-to-one) if $f(a_1) = f(a_2)\implies a_1 = a_2$.\\
- is a surjection (or onto) if every $b \in B$ has at least one pre-image in $A$.\\
- is a bijection (or one-to-one correspondence) if it is both an injection and a surjection.

\textbf{Left and Right Inverse}\\
Injective linear mappings always have a left inverse $g: W \rightarrow V$ defined $g \circ f=\mathrm{id}_V$.
Surjective linear mapping always have a right inverse $g: W \rightarrow V$ defined $f \circ g=\operatorname{id}_W$.

\textbf{Rank-Nulity}\\
Let $f: V \rightarrow W$ 
$$
\operatorname{dim} V=\operatorname{dim}(\operatorname{ker} f)+\operatorname{dim}(\operatorname{im} f) .
$$
$V$ is finite-dimensional:\\
- Then $f$ is injective if and only if $\operatorname{dim im} f=\operatorname{dim} V$. \\
- Then $f$ is surjective if and only if $\operatorname{dim} \operatorname{ker} f=\operatorname{dim} V-\operatorname{dim} W$.\\
$f$ is an isomorphism and $V$ is finite-dimensional. \\
- Then $\operatorname{dim} W=\operatorname{dim} V$. (In particular, $F^m$ and $F^n$ are isomorphic if and only if $m=n$.)\\
$V, W$ are finite-dimensional with the same dimension:\\
- $f$ is injective if and only if $f$ is surjective.

\textbf{Matrices as Linear Mapppings}\\
There is a bijection between the space of linear mappings $F^m \rightarrow F^n$ and the set of matrices with $n$ rows and $m$ columns and entries in $F$ :
\begin{align*}
\mathrm{M}: \operatorname{Hom}_F\left(F^m, F^n\right) & \stackrel{\sim}{\rightarrow} \operatorname{Mat}(n \times m ; F) \\
f & \mapsto[f] .
\end{align*}
Each linear mapping $f$ has its representing matrix $\mathrm{M}(f):=[f]$. \\
The columns of this matrix are the images under $f$ of the standard basis elements of $F^m$ 
$$
[f]:=\left(f\left(\vec{e}_1\right)\left|f\left(\vec{e}_2\right)\right| \cdots \mid f\left(\vec{e}_m\right)\right) .
$$

\textbf{Mat Mul}\\
$$
(A B)_{i k}=\sum_{j=1}^m A_{i j} B_{j k}
$$
\begin{align*}
\left(A+A^{\prime}\right) B & =A B+A^{\prime} B \\
A\left(B+B^{\prime}\right) & =A B+A B^{\prime} \\
I B & =B \\
A I & =A \\
(A B) C & =A(B C) .
\end{align*}

\textbf{Composition of Linear Mappings}\\
The composition $g \circ f: U \rightarrow W$ is the matrix product of the representing matrices of $f$ and $g$:
$$
\prescript{}{\mathcal{C}}{[g \circ f]}_{\mathcal{A}}=\prescript{}{\mathcal{C}}{[g]}_{\mathcal{B}} \circ { }_{\mathcal{B}}[f]_{\mathcal{A}}
$$

\textbf{Invertible Matrices}\\
A matrix $A$ is called invertible if and only if there exists matrices $B$ and $C$ such that $BA=I$ and $AC=I$. \\
To calculate the inverse of a matrix $A$:\\
- Write the identity matrix $I$ next to it, producing an $(n \times 2 n)$-matrix $(A \mid I)$. \\
- Bring $A$ into Echelon Form, (possibly further to into reduced echelon form: this is the identity matrix. )\\
- The inverse to $A$ is then what is standing in the right half of the $(n \times 2 n)$-matrix.

\textbf{Elementary Matrix}\\
Any square matrix that differs from the identity matrix in at most one entry.\\
All the elementary matrices with non-zero diagonlas and entries in a field, are invertible.

\textbf{Rank}\\
The column rank of a matrix $A$ is the dimension of the subspace generated by the columns of $A$. \\
Column and row rank are equal. \\
TO CALCUALTE: Count the number of non-zero rows in the echelon form of the matrix. 
$$
\operatorname{rank}(A+B) \leq \operatorname{rank} A+\operatorname{rank} B
$$

\textbf{Center of a Group}\\
The centre of a group $G$, denoted as $Z(G)$, consists of elements that commute with every element in $G$.
$$Z(G)=\{g \in G \mid \forall h \in G, g h=h g\}$$
$Z(G)$ is never empty. 
If $G$ is an abelian group, then $Z(G)=G$.

\textbf{Abstract Linear Mappings as Matrices}\\
Let $F$ be a field, $V$ and $W$ vector spaces over $F$ with ordered basis $\mathcal{A}=\left(\vec{v}_1, \ldots, \vec{v}_m\right)$ and $\mathcal{B}=\left(\vec{w}_1, \ldots, \vec{w}_n\right)$. \\U
Then each linear mapping $f: V \rightarrow W$ has a representing matrix $\prescript{}{\mathcal{B}}{[f]}_{\mathcal{A}}$ whose entries $a_{i j}$ are defined
$$
f\left(\vec{v}_j\right)=a_{1 j} \vec{w}_1+\cdots+a_{n j} \vec{w}_n \in W .
$$
i.e. the image of a basis element $\vec{v}_i\in\mathcal{A}$ of $V$ is a linear combination of the basis elements $\vec{w}_i\in\mathcal{B}$ of $W$.\\
This describes the isomorphism:
\begin{align*}
\mathrm{M}_{\mathcal{B}}^{\mathcal{A}}: \operatorname{Hom}_F(V, W) & \xrightarrow{\overset{\sim}{}}  \operatorname{Mat}(n \times m ; F) \\
f & \mapsto{ }_{\mathcal{B}}[f]_{\mathcal{A}}
\end{align*}
\textbf{Change of Basis}
$$
\prescript{}{\mathcal{B}}{\left[\mathrm{id}_V\right]}_{\mathcal{A}}
$$
Its entries $(a_{ij})$ are given by the equalities $\vec{v}_j=\sum_{i=1}^n a_{i j} \vec{w}_i$.\\
\textit{Changing between vector spaces:}\\
Let $V$ and $W$ be finite dimensional vector spaces over $F$ and let $f: V \rightarrow W$ be a linear mapping. \\
Suppose that $\mathcal{A}, \mathcal{A}^{\prime}$ are ordered basis of $V$ and $\mathcal{B}, \mathcal{B}^{\prime}$ are ordered bases of $W$. Then
$$
\prescript{}{\mathcal{B}^{\prime}}[f]_{\mathcal{A}^{\prime}}={ }_{\mathcal{B}^{\prime}}\left[\mathrm{id}_W\right]_{\mathcal{B}} \circ{ }_{\mathcal{B}}[f]_{\mathcal{A}} \circ \prescript{}{\mathcal{A}}[\mathrm{id}_V]_{\mathcal{A}^{\prime}}
$$
\textit{Changing within a vector space}\\
Let $f$ be the endomorphism $f:V\rightarrow V$, we have
$$
\prescript{}{\mathcal{A}^{\prime}}[f]_{\mathcal{A}^{\prime}}={ }_{\mathcal{A}}\left[\mathrm{id}_V\right]_{\mathcal{A}^\prime}^{-1} \circ{ }_{\mathcal{A}}[f]_{\mathcal{A}} \circ \prescript{}{\mathcal{A}}[\mathrm{id}_V]_{\mathcal{A}^{\prime}}
$$

\textbf{Similar Matrices}\\
Let $N=\prescript{}{\mathcal{B}}[f]_\mathcal{B}$ and $M=\prescript{}{\mathcal{A}}[f]_\mathcal{A}$ then if 
$$N=T^{-1}MT$$
where $T=\prescript{}{\mathcal{A}}[id_V]_\mathcal{B}$.
We say that $N$ and $M$ are similar matrices.\\
- Matrices that are similar are equivalent. \\
- Similar matrices have the same characteristic polynomial. 

\textbf{Rings}\\
A ring is a set \( R \) equipped with 
Addition satisfying:

- Commutativity: \( a + b = b + a \)

- Associativity: 
\( (a + b) + c = a + (b + c) \) 

- Identity: \(\exists 0 \in R \) such that \( a + 0 = a \) 

- Inverse: \(\exists  -a \in R \) such that \( a + (-a) = 0 \).

Multiplication satisfying:

- Associativity: \( (a \cdot b) \cdot c = a \cdot (b \cdot c) \) 

- Distributivity: \( a \cdot (b + c) = (a \cdot b) + (a \cdot c) \)

- Identity: \(\exists  1 \in R \) such that \( a \cdot 1 = a \) 

\textbf{Units of a Ring}\\
Let $R$ be a ring. An element $a \in R$ is called a unit if is invertible i.e. $\exists a^{-1} \in R$
$$
a a^{-1}=1=a^{-1} a .
$$
The set of units in a ring forms a group under multiplication called the group of units of the ring $R$ written $R^\times$.

\textbf{Integral Domains}\\
An integral domain is a non-zero commutative ring with no zero-divisors, then:\\
-  $a b=0 \Rightarrow a=0$ or $b=0$, and\\
-  $a \neq 0$ and $b \neq 0 \rightarrow a b \neq 0$\\
-  $a b=a c$ and $a \neq 0$ $\implies$ $b=c$.\\
All Fields are integral domains since a unit cannot be a zero-divisor. \\
Every finite integral domain is a field.

\textbf{Orbit - Stabiliser}\\
The orbit of an element $x$ under the action of a group $G$ is denoted as $Gx$ and is the set $\{g \cdot x \mid g \in G\}$.\\
The stabiliser of an element $x$ under the action of a group $G$, denoted as $G_x$, is the subgroup of elements in $G$ that leave $x$ fixed. $G_x=\{g \in G \mid g \cdot x=x\}$

\textbf{Polynomials}\\
Let $R$ be a ring. A polynomial over $R$ is an expression of the form
$$
P=a_0+a_1 X+a_2 X^2+\cdots+a_m X^m
$$
for some non-negative integer $m$ and elements $a_i \in R$ for $0 \leqslant i \leqslant m$. \\
The set of all polynomials over $R$ is denoted by $R[X]$. \\
In case $a_m$ is non-zero, the polynomial $P$ has degree $m$, written $\operatorname{deg}(P)$, and $a_m$ is its leading coefficient. \\
When the leading coefficient is 1 the polynomial is a monic polynomial.

\textbf{Ring of Polynomials}\\
$R[X]$ becomes a Ring called the ring of polynomials with coefficients in $R$, with the operations $+, \times$.
\begin{align*}
&\left(a_0+a_1 X+\cdots a_m X^m\right)\\
&\qquad +\left(b_0+b_1 X+\cdots b_n X^n\right)\\
&=\left(a_0+b_0\right)+\left(a_1+b_1\right) X +\cdots
\end{align*}
and
\begin{align*}
&\left(a_0+a_1 X+\cdots a_m X^m\right)\\
&\qquad\times\left(b_0+b_1 X+\cdots b_n X^n\right) \\
& =a_0 b_0+\left(a_0 b_1+a_1 b_0\right) X\\
&+\left(a_0 b_2+a_1 b_1+a_2 b_0\right) X^2+\cdots a_m b_n X^{m+n}
\end{align*}
The zero and the identity of $R[X]$ are the zero and identity of $R$, respectively.\\
The elements of $R$ are just polynomials of degree 0. These are constant polynomials. \\
$R$ is commutative, then so too is $R[X]$.\\
If $R$ is a ring with no zero-divisors, then $R[X]$ has no zero-divisors and $\operatorname{deg}(P Q)=$ $\operatorname{deg}(P)+\operatorname{deg}(Q)$ for non-zero $P, Q \in R[X]$.
If $R$ is an integral domain then so is $R[X]$.\\
Let $R$ be an integral domain and let $P, Q \in R[X]$ with $Q$ monic. 
Then there exists unique $A, B \in R[X]$ such that $$P=A Q+B$$ and $\operatorname{deg}(B)<\operatorname{deg}(Q)$ or $B=0$.

\textbf{Polynomial Roots}\\
Let $R$ be a commutative ring, let $\lambda \in R$ and $P(X) \in R[X]$. Then $\lambda$ is a root of $P(X)$ if and only if $(X-\lambda)$ divides $P(X)$.\\
Let $R$ be an integral domain. Then $P[X]$ has at most $\operatorname{deg}(P)$ roots in $R$.

\textbf{Algebraic Closure}\\
A field $F$ is algebraically closed if each non-constant polynomial with coefficients in our field has a root in our field $F$.\\
If $F$ is an algebraically closed field, then every non-zero polynomial decomposes into linear factors
$$
P=c\left(X-\lambda_1\right) \cdots\left(X-\lambda_n\right)
$$
with $n \geqslant 0, c \in F^{\times}$and $\lambda_1, \ldots, \lambda_n \in F$.\\
This decomposition is unique up to reordering the factors.

\textbf{Fundamental Theorem of Algebra}\\
The field of complex numbers $\mathbb{C}$ is algebraically closed.

\textbf{Rings Homomorphisms}\\
Let $R$ and $S$ be rings. \\
A mapping $f: R \longrightarrow S$ is a ring homomorphism if the following hold for all $x, y \in R$ 
\begin{align*}
f(x+y) & =f(x)+f(y) \\
f(x y) & =f(x) f(y)
\end{align*}
Then:\\
- $f\left(0_R\right)=0_S$\\
- $f(-x)=-f(x)$;\\
- $f(x-y)=f(x)-f(y)$;\\
- $f(m x)=m f(x)$,\\
- $f\left(x^n\right)=(f(x))^n$\\
$f$ is injective if and only if $\operatorname{ker} f=\{0\}$.

\textbf{Ideals}\\
A subset $I$ of a ring $R$ is an ideal, written $I \unlhd R$, if the following hold:\\
1. $I \neq \emptyset$;\\
2. $I$ is closed under subtraction;\\
3. for all $i \in I$ and $r \in R$ we have $r i$, $ir \in I$ i.e. $I$ is closed under multiplication by elements of $R$.\\
In any ring $R,\{0\}$ and $R$ are ideals of $R$. \\
The intersection of ideals of a ring $R$ is an ideal of $R$.\\
Let $I$ and $J$ be ideals of a ring $R$. Then
$$
I+J=\{a+b: a \in I, b \in J\}
$$
is an ideal of $R$.

\textbf{Generting Ideals}\\
Let $R$ be a commutative ring and let $T \subset R$. 
Then the ideal of $R$ generated by $T$ is 
\begin{align*}
&{ }_R\langle T\rangle=\{r_1 t_1+\cdots+r_m t_m: t_1, \ldots, t_m \in T, \\
&\qquad\qquad\qquad\qquad\qquad\qquad r_1, \ldots, r_m \in R\},
\end{align*}
together with the zero element in the case $T=\emptyset$. \\
${ }_R\langle T\rangle$ is the *smallest* ideal of $R$ that contains $T$.\\
- Let $m \in \mathbb{Z}$. Then ${ }_{\mathbb{Z}}\langle m\rangle=m \mathbb{Z}$.\\
- Let $P \in \mathbb{R}[X]$. Then ${}_{\mathbb{R}[X]}\langle P\rangle=\{A P: A \in \mathbb{R}[X]\}=\{Q: P$ divides $Q$ in $\mathbb{R}[X]\}$.

\textbf{Principle Ideals}\\
An ideal $I$ of $R$ is called a principal ideal if $I=\langle t\rangle$ for some $t \in R$ i.e. it $I$ is generated by one element of $R$.

\textbf{Kernal of a Ring Homomorphism}\\
Let $f: R \rightarrow S$ be a ring homomorphism. 
$$
\text { ker } f=\left\{r \in R: f(r)=0_S\right\} \text {. }
$$

\textbf{Subrings}\\
Let $R$ be a ring. A subset $R^{\prime}$ of $R$ is a subring of $R$ if $R^{\prime}$ itself is a ring under the operations of addition and multiplication defined in $R$.\\
- It is not true that the intersection of two subrings of $R$ is a subring of $R$.\\
QUICK CHECK\\
$R^{\prime}$ is a subring if and only if\\
1) $R^{\prime}$ has a multiplicative identity\\
2) $R^{\prime}$ is closed under subtraction: \\
$a, b \in R^{\prime} \rightarrow a-b \in R^{\prime}$\\
3) $R^{\prime}$ is closed under multiplication.

\textbf{Subrings and Homomorphisms}\\
Let $f: R \rightarrow S$ be a Ring Homomorphism.\\
1) If $R^{\prime}$ is a subring of $R$ then $f\left(R^{\prime}\right)=\operatorname{im} f$ is a subring of $S$.\\ 
2) Assume that $f\left(1_R\right)=1_S$. Then if $x$ is a unit in $R, f(x)$ is a unit in $S$ and $(f(x))^{-1}=f\left(x^{-1}\right)$. In this case $f$ restricts to a group homomorphism $\left.f\right|_{R^{\times}}: R^{\times} \rightarrow S^{\times}$.

\textbf{Equivalence Relation}\\
$\sim$ is an equivalence relation on $X$ when for all elements $x, y, z \in X$ we have:\\
1) Reflexivity: $x \sim x$;\\
2) Symmetry: $x \sim y \iff y R x$;\\
3) Transitivity: $x \sim y, \,y \sim z \implies x \sim z$.

\textbf{Equivalence Class}\\
Suppose that $\sim$ is an equivalence relation on a set $X$. For $x \in X$ the set $E(x):=$ $\{z \in X: z \sim x\}$ is called the equivalence class of $x$. \\
An element of an equivalence class is called a representative of the class. \\
A subset $Z \subseteq X$ containing precisely one element from each equivalence class is called a system of representatives for the equivalence relation.\\
For $x, y \in X$ the following are equivalent:\\
1) $x \sim y$;\\
2) $E(x)=E(y)$;\\
3) $E(x) \cap E(y) \neq \emptyset$.

\textbf{Set of Equivalence Classes}\\
Given an equivalence relation $\sim$ on the set $X$, we denote the set of equivalence classes
$$
(X / \sim):=\{E(x): x \in X\}
$$
Each element of $X$ must belong to some equivalence class, impliing the surjection
$$can : X \rightarrow(X / \sim), x \mapsto E(x)$$

\textbf{Well-Defined Mappings}\\
Given $g:(X / \sim) \rightarrow Z$, and $f: X \rightarrow Z$. \\
$g$ is \textit{well-defined} and only if $$x \sim y \Rightarrow f(x)=f(y)$$

\textbf{Cosets of a Ring}\\
Let $I \unlhd R$ be an ideal in a Ring $R$. A coset of $I$ in $R$ is the set
$$
x+I:=\{x+i: i \in I\} \subseteq R
$$
Cosets are also defined by the equivalence relation $x \sim y \Leftrightarrow x-y \in I$.

\textbf{Factor Rings}\\
Let $I \unlhd R$ be an ideal in a ring $R$. 
$R / I$ is the factor ring of $R$ by $I$ 
$$R/I = \{r + I \mid r \in R\}$$
It describes is the set of cosets of $R$ with $I$. We have
This becomes an ring with
$$
\begin{aligned}
(x+I) {+}(y+I) & =(x+y)+N \\
(x+I) \cdot(y+I)&=x y+I\\
\end{aligned}
$$
for all $x, y \in R$ with $0+I$ as the additive identity and $-x+I$ as the inverse of $x+I$.

\textbf{Universal Property of Factor Rings}\\
Let $R$ be a ring and $I$ an ideal of $R$.\\
1) The mapping $can : R \rightarrow R / I$ sending $r$ to $r+I$ for all $r \in R$ is a surjective ring homomorphism where $I = \operatorname{ker}(can)$.\\
2) If $f: R \rightarrow S$ is has $I \subseteq \operatorname{ker} f$, then there is a unique ring homomorphism $\bar{f}: R / I \rightarrow S$ such that $f=\bar{f} \circ can$.
\begin{center}
    \includegraphics[width=0.3\textwidth]{Pasted image 20240224112536.png}
\end{center}

\textbf{First Iso Theorem for Rings}\\
Let $R$ and $S$ be Rings. Then every $f: R \longrightarrow S$ induces a ring Isomorphism
$$
\bar{f}: R / \operatorname{ker} f \xrightarrow{\sim} \operatorname{im} f .
$$

\textbf{Modules}\\
A (left) module $M$ over a ring $R$ is a pair consisting of an Abelian group $M=$ $(M, \dot{+})$ and a mapping
\begin{align*}
R \times M & \rightarrow M \\
(r, a) & \mapsto r a
\end{align*}
such that for all $r, s \in R$ and $a, b \in M$ 
\begin{align*}
r(a+b) & =(r a) \dot{+}(r b) \\
(r+s) a & =(r a) \dot{+}(s a) \\
r(s a) & =(r s) a \\
1_R a & =a\\
0_R a&=0_M\\
r 0_M&=0_M\\
(-r) a&=r(-a)=-(r a)
\end{align*}

\textbf{Direct Sum of Modules}\\
Given a Ring $R$ and $R$-modules $M_1, \ldots, M_n, M$, the cartesian product $M_1 \times M_2 \times$ $\cdots \times M_n\in M$ when 
\begin{align*}
&\left(a_1, \ldots, a_n\right)+\left(b_1, \ldots, b_n\right)\\
&=\left(a_1+b_1, \ldots, a_n+b_n\right)
\end{align*}
$$
r\left(a_1, \ldots, a_n\right)=\left(r a_1, \ldots, r a_n\right)
$$
for all $r \in R$ and $a_i, b_i \in M$. \\
This is denoted $M_1 \oplus \cdots \oplus M_n$ and called the direct sum. 

\textbf{Sub-module}\\
A non-empty subset $M^{\prime}$ of an $R$-module $M$ is a submodule if $M^{\prime}$ is an $R-$ module with respect to the operations $M$ restricted to $M^{\prime}$.\\
- Let $T \subseteq M$. Then ${ }_R\langle T\rangle$ is the smallest submodule of $M$ that contains $T$.\\
- The intersection of any collection of submodules of $M$ is a submodule of $M$.\\
- Let $M_1$ and $M_2$ be submodules of $M$. 
$$
M_1+M_2=\left\{a+b: a \in M_1, b \in M_2\right\}
$$
is a submodule of $M$\\
QUICK CHECK\\
1) $0_M \in M^{\prime}$\\
2) $a, b \in M^{\prime} \Rightarrow a-b \in M^{\prime}$\\
3) $r \in R, a \in M^{\prime} \Rightarrow r a \in M^{\prime}$.

\textbf{Cosets of a Module}\\
Let $M$ an $R$-module and $N$ a submodule of $M$. For each $a \in M$ the coset of $a$ with respect to $N$ in $M$ is
$$
a+N=\{a+b: b \in N\}
$$

\textbf{Factor Module}\\
Let $N$ be a sub-module of a $R$-module $M$. \\
$M / N$ is the factor module of $M$ by $N$ := 
$$M/N = \{m + N \mid m \in M\}$$
It describes the set of all cosets of $N$ in $M$. \\
This becomes an $R$-module with
$$
\begin{aligned}
(a+N) \dot{+}(b+N) & =(a+b)+N \\
r(a+N) & =r a+N
\end{aligned}
$$
for all $a, b \in M, r \in R$. 

\textbf{Universal Property of Factor Modules}\\
Let $L,M$ be $R$-modules, and $N$ a Submodule of $M$.\\
1) The mapping $can : M \rightarrow M / N$ sending a to $a+N$ for all $a \in M$ is a surjective $R$-homomorphism with kernel $N$.\\
2) If $f: M \rightarrow L$ is an $R$-homomorphism with $N \subseteq \operatorname{ker} f$, then there is a unique homomorphism $\bar{f}: M / N \rightarrow L$ such that $f=\bar{f} \circ can$.

\textbf{First Isomorphism Theorem for Modules}\\
Let $M$ and $N$ be $R$-modules. 
Then every $f: M \longrightarrow N$ induces an $R$-isomorphism
$$
\bar{f}: M / \operatorname{ker} f \xrightarrow{\sim} \operatorname{im} f .
$$

\textbf{Multilinear Form}\\
Let $U_1, U_2, \ldots, U_n, W$ be vector spaces over a field $F$, then a map
$$
H: U_1 \times U_2 \times \cdots \times U_n \rightarrow W
$$
is multilinear if it is linear in each of its entries separately.\\
In the case $n=2$ this is exactly the definition of a bilinear form.\\
A multilinear form is alternating if it vanishes on every $n$-tuple of elements of $U$ that has at least two entries equal.
This is the same as writing, for any $\sigma \in \mathfrak{S}_n$
$$
H\left(v_{\sigma(1)}, \ldots, v_{\sigma(n)}\right)=\operatorname{sgn}(\sigma) H\left(v_1, \ldots, v_n\right)
$$

\textbf{Determinant}\\
The determinant is a mapping from $Mat(n,R)\rightarrow R$ where $n=0\implies \det(A) = 1$.\\
\textit{Multilinear Form Characterisation}\\
Let $F$ be a Field. The mapping
\begin{align*}
&\operatorname{det}: F^n \times \cdots \times F^n \rightarrow F\left(v_1, \ldots, v_n\right) \\
&\qquad\qquad\qquad\qquad\mapsto \operatorname{det}\left(v_1|\cdots| v_n\right)
\end{align*}
is the \textit{unique} \textit{alternating} multilinear form on n-tuples of column vectors with values in $F$ that takes the value $1_F$ on the identity matrix.\\
\textit{Leibniz Characterisation}\\
\[
A \mapsto \operatorname{det}(A)=\sum_{\sigma \in \mathfrak{S}_n} \operatorname{sgn}(\sigma) a_{1 \sigma(1)} \ldots a_{n \sigma(n)}
\]
The sum is over all permutations of $n$.
\textit{Laplace’s Expansion of the Determinant}\\
Let $A=\left(a_{i j}\right)$. For a fixed $i$ the $i$-th row expansion of the determinant is
\[
\operatorname{det}(A)=\sum_{j=1}^n a_{i j} C_{i j}
\]
and for a fixed $j$ the $j$-th column expansion of the determinant is
\[
\operatorname{det}(A)=\sum_{i=1}^n a_{i j} C_{i j}
\]
where the $(i, j)$ cofactor of $A$ is $$C_{i j}=(-1)^{i+j} \operatorname{det}(A\langle i, j\rangle)$$ where $A\langle i, j\rangle$ is the matrix obtained from $A$ be deleting row $i$ and column $j$.\\
\textit{Multiplicativity}\\
\[
\operatorname{det}(A B)=\operatorname{det}(A) \operatorname{det}(B) \text {. }
\]
\textit{Determinantal Criterion for Invertibility} \\
The determinant of a square matrix with entries in a field $F$ is non-zero if and only if the matrix is invertible. \\
A square matrix with entries in a commutative ring $R$ is invertible if and only if its determinant is a unit in $R$. \\
\textit{Inverse of the Determinant}\\
If $A$ is invertible then $\operatorname{det}\left(A^{-1}\right)=\operatorname{det}(A)^{-1}$. 
If $B$ is a square matrix $B$ then
\[
\operatorname{det}\left(A^{-1} B A\right)=\operatorname{det}(B)
\]
\textit{Determinant of an Endomorphism}\\
The determinant of an representative matrix ${ }_{\mathcal{A}}[f]_{\mathcal{A}}$ is independent of the choice of basis $\mathcal{A}$. Therefore the determinant is in fact defined only by the endomorphism $f$. \\
\textit{Transpose}
\[
\operatorname{det}\left(A^{\top}\right)=\operatorname{det}(A)
\]
\textit{Cramer's Rule}\\
\[
A \cdot \operatorname{adj}(A)=(\operatorname{det} A) I_n
\]
\textit{Jacobi's Formula}\\
Let $A=\left(a_{i j}\right)$ where the coefficients $a_{i j}=a_{i j}(t)$ are functions of $t$. Then
\[
\frac{d}{d t} \operatorname{det} A=\operatorname{Tr} \operatorname{Adj} A \frac{d A}{d t} .
\]
\textit{Notes}\\
- $\operatorname{rk} A < n \implies \det A = 0$\\

\textbf{Adjugate}\\
For $A\in Mat(n, R)$ for a commutative ring $R$. 
$$\operatorname{adj}(A)_{i j}=C_{j i}$$ where $C_{j i}$ is the $(j, i)$-cofactor.

\textbf{EigenStuff}\\
Let $f: V \rightarrow V$ be an endomorphism of an $F$-vector space $V$. $A$ scalar $\lambda \in F$ is an \textit{eigenvalue} of $f$ if and only if there exists a non-zero vector $\vec{v} \in V$ such that $f(\vec{v})=\lambda \vec{v}$. Each such vector is called an \textit{eigenvector} of $f$ with eigenvalue $\lambda$. For any $\lambda \in F$, the \textit{eigenspace} of $f$ with eigenvalue $\lambda$ is
$$
E(\lambda, f)=\{\vec{v} \in V: f(\vec{v})=\lambda \vec{v}\}
$$
\textit{Properties of EigenStuff}\\
Each endomorphism of a non-zero finite dimensional vector space over an algebraically closed field has an eigenvalue.\\
Eigenvectors are linearly independent. 

\textbf{Characteristic Polynomial}
$$\chi_A(x):=\operatorname{det}\left(x I_n-A\right)$$
The eigenvalues of the linear mapping $A: F^n \rightarrow F^n$ are exactly the roots of the characteristic polynomial $\chi_A$.\\
A matrix is nilpotent $\iff$ $\chi_A(x)=x^n$\\
Similar matrices (including transposes !) have the same characteristic polynomial.\\
The constant term of the characteristic polynomial is $(-1)^n \operatorname{det} A$.

\textbf{Triangularisability}\\
Let $f: V \rightarrow V$ be an endomorphism of a finite dimensional $F$-vector space $V$. $f$ is triangularisable if either of the following statements are true\\
(1) The vector space $V$ has an ordered basis $\mathcal{B}=\left(\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\right)$ such that
$$
\begin{aligned}
f\left(\vec{v}_1\right) & =a_{11} \vec{v}_1, \\
f\left(\vec{v}_2\right) & =a_{12} \vec{v}_1+a_{22} \vec{v}_2, \\
& \vdots \\
f\left(\vec{v}_n\right) & =a_{1 n} \vec{v}_1+a_{2 n} \vec{v}_2+\cdots+a_{n n} \vec{v}_n \in V
\end{aligned}
$$
(so that the first basis vector $\vec{v}_1$ is an eigenvector, with eigenvalue $a_{11}$ ) or equivalently such that the $n \times n$ matrix $_{\mathcal{B}}[f]_{\mathcal{B}}=\left(a_{i j}\right)$ representing $f$ with respect to $\mathcal{B}$ is upper triangular.\\
(2) The characteristic polynomial $\chi_f(x)$ of $f$ decomposes into linear factors in $F[x]$.\\
(3) $A= [f]$ is conjugate to an upper triangular matrix $B$, with $P^{-1} A P=B$ for an invertible matrix $P$.\\
NOTE: Any endomorphism of a $\mathbb{C}$-vector space is triangularisable.

\textbf{Diagonalisability}\\
An endomorphism $f: V \rightarrow V$ of an F-vector space $V$ is diagonalisable if and only if there exists a basis of $V$ consisting of eigenvectors of $f$. If $V$ is finite dimensional then this is the same as saying that there exists an ordered basis $\mathcal{B}=\left\{\vec{v}_1, \ldots, \vec{v}_n\right\}$ such that corresponding matrix representing $f$ is diagonal.

\textbf{Cayley-Hamilton}\\
Let $A \in \operatorname{Mat}(n ; R)$ be a square matrix on a commutative ring $R$. Then evaluating its characteristic polynomial $\chi_A(x) \in R[x]$ at the matrix A gives zero.

\textbf{Inner Product}\\
Let $V$ be a Vector Space over $\mathbb{R}$. A real inner product on $V$ is a mapping
$$
(-,-): V \times V \rightarrow \mathbb{R}
$$
such that for all $\vec{x}, \vec{y}, \vec{z} \in V$ and $\lambda, \mu \in \mathbb{R}$ :\\
1) $(\lambda \vec{x}+\mu \vec{y}, \vec{z})=\lambda(\vec{x}, \vec{z})+\mu(\vec{y}, \vec{z})$\\
2) $(\vec{x}, \vec{y})=(\vec{y}, \vec{x})$\\
3) $(\vec{x}, \vec{x}) \geqslant 0$, with equality if and only if $\vec{x}=\overrightarrow{0}$\\
- A real inner product space is a Symmetric Bilinear Form, i.e. it is linear in both variables.\\
- A finite-dimensional real inner product space is a Euclidean Vector Space.\\
- Every finite dimensional inner product space has an orthonormal basis. \\
- The stander real inner product is the dot product.\\
Complex inner products are defined on Vector Spaces over $\mathbb{C}$ and map to $\mathbb{C}$. 
The differences from a real iner product are if $\lambda, \mu \in \mathbb{C}$ :\\
2) $(\vec{x}, \vec{y})=\overline{(\vec{y}, \vec{x})}$\\
NOTE: A complex inner product is NOT linear in the second variable.\\
The standard inner product for complex inner product spaces is
$$
(\vec{v}, \vec{w})=v_1 \overline{w_1}+v_2 \overline{w_2}+\cdots+v_n \overline{w_n}
$$
An inner product space is any vector space endowed with an inner product.\\

\textbf{Inner Product Norm}\\
In a real or complex inner product space the length or inner product norm $\|\vec{v}\| \in \mathbb{R}$ of a vector $\vec{v}$ is defined as the non-negative square root
$$
\|\vec{v}\|=\sqrt{(\vec{v}, \vec{v})}
$$

\textbf{Orthonormal Family and Basis}\\
A family $\left(\vec{v}_i\right)_{i \in I}$ for vectors from an inner product space is an orthonormal family if all the vectors $\vec{v}_i$ have length 1 and if they are pairwise orthogonal to each other
$$
\left(\vec{v}_i, \vec{v}_j\right)=\delta_{i j}
$$
An orthonormal family that is a basis is an orthonormal basis.

\textbf{Orthogonality}\\
Two vectors $\vec{v}, \vec{w}$ are orthogonal 
$$
\vec{v} \perp \vec{w}
$$
if and only if $(\vec{v}, \vec{w})=0$. We say that $\vec{v}$ and $\vec{w}$ are at right-angles to each other. \\
We write $S \perp T$ as a shorthand for $\vec{v} \perp \vec{w}$ for all $\vec{v} \in S$ and $\vec{w} \in T$.

\textbf{Orthogonal Projection}\\
The orthogonal projection from $V$ onto $U$ is the mapping
$$
\pi_U: V \rightarrow V
$$
that sends $\vec{v}=\vec{p}+\vec{r}$ to $\vec{p}$.\\
1) $\pi_U$ is a linear mapping with $\operatorname{im}\left(\pi_U\right)=U$ and $\operatorname{ker}\left(\pi_U\right)=U^{\perp}$.\\
2) If $\left\{\vec{v}_1, \ldots, \vec{v}_n\right\}$ is an Orthonormal Basis of $U$, then $\pi_U$ is given by the following formula for all $\vec{v} \in V$
$$
\pi_U(\vec{v})=\sum_{i=1}^n\left(\vec{v}, \vec{v}_i\right) \vec{v}_i
$$
3) $\pi_U^2=\pi_U$, that is $\pi_U$ is an idempotent.

\textbf{Orthogonal Complement}\\
Let $V$ be an Inner Product Space and let $T \subseteq V$ be an arbitrary subset. Define
$$
T^{\perp}=\{\vec{v} \in V: \vec{v} \perp \vec{t} \text { for all } \vec{t} \in T\},
$$
calling this set the orthogonal to $T$.
If $T$ is a subspace it is the orthogonal complement to $V$. 

\textbf{Orthogonal Matrix}\\
An orthogonal matrix is any $P\in Mat(n, \mathbb{R}$ such that $P^{\top} P=$ $I_n$. 
In other words, an orthogonal matrix is a square matrix $P$ with real entries such that $P^{-1}=P^{\top}$.

\textbf{Unitary Matrix}\\
An unitary matrix is an $(n \times n)$-matrix $P$ with complex entries such that $\bar{P}^{\top} P=I_n$. \\
i.e. $P^{-1}=$ $\bar{P}^{\top}$.

\textbf{Gram-Schmidt Process}\\
Given an arbitrary linearly independent ordered subset $\vec{v}_1, \vec{v}_2, \ldots$ of an inner product space $V$. \\
Our aim is to produce the elements of $(\vec{w}_i)$, an orthonormal family in $V$. \\
1. Take the first element $\vec{v}_1$ and normalize it to have length 1 . Let this be the first element $\vec{w}_1$.\\
2. For each subsequent vector $\vec{v}_i$ from the subset:\\
	- Subtract the orthogonal projection of $\vec{v}_i$ onto the space $\langle \vec{w}_1, \vec{w}_2, \ldots, \vec{w}_{i-1}\rangle$.\\
	- Normalize the resulting vector to have length 1 . Let this be the $i$ th element $\vec{w}_i$ of the orthonormal family.\\
Repeat this process until for all $\vec{v_i}$.

\textbf{Adjoint Endomorphisms}\\
Let $V$ be an Inner Product Space. Then $T, T^*: V \rightarrow V$ are adjoint if for all $\vec{v}, \vec{w} \in V$
$$
(T \vec{v}, \vec{w})=(\vec{v}, T^* \vec{w})
$$
Any endomorphism always has an adjoint.

\textbf{Self-Adjoint}\\
Let $V$ be an inner product space, $T: V \rightarrow V$ is self-adjoint if $T^*=T$.
1) Every eigenvalue of $T$ is real.\\
2) If $\lambda$ and $\mu$ are distinct eigenvalues of $T$ with corresponding eigenvectors $\vec{v}$ and $\vec{w}$, then $(\vec{v}, \vec{w})=0$.\\
3) T has an eigenvalue.

\textbf{Hermitian Matrices}\\
$A\in Mat(n, \mathbb{R})$ describes a self-adjoint mapping on the standard inner product space $\mathbb{R}^n$ precisely when $A^{\top}=A$. \\
$A\in Mat(n, \mathbb{C})$ describes a self-adjoint mapping on the standard inner product space $\mathbb{C}^n$ precisely when $A=\bar{A}^{\top}$ holds. \\
Such matrices are called hermitian.

\textbf{Conjugate Transpose}\\
The conjugate transpose $\bar{A}^T$ is the matrix obtained from $A$ by first conjugating each entry and then transposing the resulting matrix. 

\textbf{Raleigh Quotient}\\
$V$ is a finite dimensional real Inner Product Space. 
The Raleigh Quotient is the real-valued function defined 
\begin{align*}
R: V \backslash\{\overrightarrow{0}\} &\rightarrow \mathbb{R}\\
\vec{v} &\mapsto R(\vec{v})=\frac{(T \vec{v}, \vec{v})}{(\vec{v}, \vec{v})}
\end{align*}

\textbf{Spectral Theorem}\\
\textit{For Self-Adjoint Endomorphisms}\\
Let $V$ be a finite dimensional Inner Product Space and let $T: V \rightarrow V$ be a self-adjoint linear mapping. 
Then $V$ has an Orthonormal Basis consisting of eigenvectors of $T$.\\
\textit{For Real Symmetric Matrices}\\
Let $A$ be a real $(n \times n)$ symmetric matrix. Then there is an $(n \times n)$-orthogonal matrix $P$ such that
$$
P^{\top} A P=P^{-1} A P=\operatorname{diag}\left(\lambda_1, \ldots, \lambda_n\right)
$$
where $\lambda_1, \ldots, \lambda_n$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of the characteristic polynomial of $A$.\\
\textit{For Hermitian Matrices}\\
Let A be a $(n \times n)$-hermitian matrix. Then there is an $(n \times n)$-unitary matrix $P$ such that
$$
\bar{P}^{\top} A P=P^{-1} A P=\operatorname{diag}\left(\lambda_1, \ldots, \lambda_n\right)
$$
where $\lambda_1, \ldots, \lambda_n$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of the characteristic polynomial of $A$.

\textbf{Exponential Mapping}\\
$$
\begin{aligned}
\exp : \operatorname{Mat}(n ; \mathbb{C}) & \rightarrow \operatorname{Mat}(n ; \mathbb{C}) \\
A & \mapsto \sum_{k=0}^{\infty} \frac{1}{k !} A^k
\end{aligned}
$$
If $A \in \operatorname{Mat}(n ; \mathbb{C})$ is a square matrix and $\vec{c} \in \mathbb{C}^n$ a column vector, then there exists exactly one differentiable mapping $\gamma: \mathbb{R} \rightarrow \mathbb{C}^n$ with initial value $\gamma(0)=\vec{c}$ and which satisfies $\dot{\gamma}(t)=A \gamma(t)$ for all $t \in \mathbb{R}$ : it is the mapping
$$
\gamma(t)=\exp (t A) \vec{c} .
$$

\textbf{Generilsed Eigenspace}\\
The generalized eigenspace of $\phi$ with eigenvalue $\lambda_i$, is the subspace of $V$ defined
$$
E^{\text {gen}}\left(\lambda_i, \phi\right)=\ker\left(\phi-\lambda_i \operatorname{id}_V\right)^n
$$
\textit{Algebraic Multiplicity}\\
Defined $\dim (E^{\text {gen}}(\lambda_i, \phi))$. Can also be calculated from the the power of the factor of $\chi_A$ for each $\lambda_i$.\\
\textit{Geometric Multiplicity}\\
Defined $\dim (E\left(\lambda_i, \phi\right))=\dim(\ker(A-\lambda_i I))$\\
NOTE: Algebraic multiplicity $\geq$ Geometric multiplicity.

\textbf{Bezouts identity for polynomials}\\
For a characteristic polynomial
$$
\chi_\phi(x)=\prod_{i=1}^s\left(x-\lambda_i\right)^{a_i} \in F[x]
$$
where each $a_i$ is a positive integer, $\lambda_i \neq \lambda_j$ for $i \neq j$, and $\lambda_i$ are e.v.s of $\phi$. For each $1 \leq j \leq s$ define
$$
P_j(x)=\prod_{\substack{i=1 \\ i \neq j}}^s\left(x-\lambda_i\right)^{a_i}
$$
There exists polynomials $Q_j(x) \in F[x]$ such that
$$
\sum_{j=1}^s P_j(x) Q_j(x)=1
$$

\textbf{JNF}\\
Let $F$ be an algebraically closed field. Let $V$ be a finite dimensional vector space and let $\phi: V \rightarrow V$ be an endomorphism of $V$ with char. polynomial
$$
\chi_\phi(x)=\left(x-\lambda_1\right)^{a_1}\left(x-\lambda_2\right)^{a_2} . .\left(x-\lambda_s\right)^{a_s}$$
$$
a_i \geq 1, \sum_{i=1} a_i=n
$$

For distinct $\lambda_1, \lambda_2, \ldots, \lambda_s \in F$. Then there exists an ordered basis $\mathcal{B}$ of $V$ such that the matrix of $\phi$ with respect to the block $\mathcal{B}$ is block diagonal with Jordan blocks on the diagonal, 
\begin{align*}
{ }_\mathcal{B}[\phi]_{\mathcal{B}}=&\operatorname{diag}(J(r_{11}, \lambda_1), \ldots, J(r_{1 m_1}, \lambda_1)\\
&J(r_{21}, \lambda_2), \ldots, J(r_{s m_s}, \lambda_s))
\end{align*}

with $r_{11}, \ldots, r_{1 m_1}, r_{21}, \ldots, r_{s m_s} \geq 1$ such that
$$
a_i=r_{i_1}+r_{i_2}+\cdots+r_{i m_i} \quad(1 \leq i \leq s)
$$

\textbf{Building JNF}\\
Algebraic multiplicty denotes the size of Jordan Blocks. 
Geometric multiplicty denotes the number of boxes in each block. If $\alpha_i>3$ then we must calculate the generilised eigenspaces to find the size of each box. 

\textbf{Kronecker Delta}
$\delta_j^i= \begin{cases}1 & \text { if } i=j \\ 0 & \text { if } i \neq j\end{cases}$

\textbf{Euler Identity}
$e^{iz} = \cos z + i \sin z$

\textbf{Rotation Matrix}
$$
\left(
\begin{array}{cc}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta
\end{array}
\right)
$$

\textbf{Disciminant}\\
$b^2 - 4ac$\\
$> 0$: two distinct real roots.\\
$= 0$: exactly one real root.\\
$< 0$: no real roots. 

\textbf{Mod}\\
- $\mathbb{Z} / m \mathbb{Z}$ is a ring. \\
- $\bar{a}=a+m \mathbb{Z}\in\mathbb{Z} / m \mathbb{Z}$ is a congruence class.\\
- $\mathbb{Z} / m \mathbb{Z}=\{\overline{0}, \overline{1}, \ldots, \overline{m-1}\}$\\
- $|\mathbb{Z} / m \mathbb{Z}|=m$\\
- $\bar{a}+\bar{b}=\overline{a+b}$\\
- $\bar{a} \cdot \bar{b}=\overline{a b}$\\
- $\mathbb{Z} / m \mathbb{Z}$ is an integral domain if and only if $m$ is prime.

\textbf{Symmetric Group}\\
$\mathfrak{S}_n := $ all permutations of the set $\{1,2, \ldots, n\}$ under composition.
It has $n !$ elements.\\
A \textit{transposition} is a permutation that only swaps two elements with $l(\sigma) = 2|j-i|-1$\\
An \textit{inversion} is a pair $(i, j)$ such that $1 \leqslant i<j \leqslant n$ and $\sigma(i)>\sigma(j)$.\\
Lenght is the number of inversions in a permutation. (i.e. number of crossings in a diagram)
$$
\operatorname{sgn}(\sigma)=(-1)^{\ell(\sigma)}
$$

\textbf{Matrix Stuff}
\begin{itemize}[label=\textbullet, labelsep=0.3em, leftmargin=1em]
\item $(A B)^T=B^T A^T$
\item $\operatorname{tr}(c A)=c \operatorname{tr}(A)$ for a scalar $c$.
\item $\operatorname{tr}(A B C)=\operatorname{tr}(B C A)=\operatorname{tr}(C A B)$.
\item Similar matrices have the same trace. 
\item $\overline{(\mathrm{A}+\mathrm{B})}=\overline{\mathrm{A}}+\overline{\mathrm{B}}$
\item $\overline{(\mathrm{AB})}=\overline{\mathrm{B}} \cdot \overline{\mathrm{A}}$
\item $\overline{(\mathrm{kA})}=\mathrm{k} \cdot \overline{\mathrm{A} .}$
\end{itemize}

\textbf{Counter Examples}
\begin{itemize}[label=\textbullet, labelsep=0.3em, leftmargin=1em]
    \item Ring of real quaternions are a division ring
    \item $(\mathbb{Z} / 6 \mathbb{Z})^{\times}=\{1,5\} \text {, which is not cyclic }$
\end{itemize}

\textbf{Trig.}
\begin{itemize}[label=\textbullet, labelsep=0.3em, leftmargin=1em]
    \item $\sin (\theta \pm \phi)=\sin \theta \cos \phi \pm \cos \theta \sin \phi$
    \item $\cos (\theta \pm \phi)=\cos \theta \cos \phi \mp \sin \theta \sin \phi$
    \item $\tan (\theta \pm \phi)=\frac{\tan \theta \pm \tan \phi}{1 \mp \tan \theta \tan \phi}$
    \item $\sin(2\theta) = 2 \sin \theta \cdot \cos \theta$
    \item $\cos(2\theta) = \cos^2 \theta - \sin^2 \theta$
    \item $\tan(2\theta) = \frac{2 \tan \theta}{1 - \tan^2 \theta}$
    \item $\sin \theta+\sin \phi=2 \sin \left(\frac{\theta+\phi}{2}\right) \cos \left(\frac{\theta-\phi}{2}\right)$
    \item $\sin \theta-\sin \phi=2 \cos \left(\frac{\theta+\phi}{2}\right) \sin \left(\frac{\theta-\phi}{2}\right)$
    \item $\cos \theta+\cos \phi=2 \cos \left(\frac{\theta+\phi}{2}\right) \cos \left(\frac{\theta-\phi}{2}\right)$
    \item $\cos \theta-\cos \phi=-2 \sin \left(\frac{\theta+\phi}{2}\right) \sin \left(\frac{\theta-\phi}{2}\right)$
    \item $\sin \theta \sin \phi=\frac{[\cos (\theta-\phi)-\cos (\theta+\phi)]}{2}$
    \item $\cos \theta \cos \phi=\frac{[\cos (\theta-\phi)+\cos (\theta+\phi)]}{2}$
    \item $\sin \theta \cos \phi=\frac{[\sin (\theta+\phi)+\sin (\theta-\phi)]}{2}$
    \item $\cos \theta \sin \phi=\frac{[\sin (\theta+\phi)+\sin (\theta-\phi)]}{2}$
\end{itemize}

\end{multicols*}
\end{document}